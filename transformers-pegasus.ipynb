{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998b4acd-e9c0-4b1c-b78a-c0b6f87ddd29",
   "metadata": {},
   "source": [
    "## ËØÑ‰ª∑ÊåáÊ†á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b41426-78c2-48f3-b7bf-eeefb1bbb64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:27.181991Z",
     "iopub.status.busy": "2023-01-13T08:16:27.181040Z",
     "iopub.status.idle": "2023-01-13T08:16:27.201467Z",
     "shell.execute_reply": "2023-01-13T08:16:27.200989Z",
     "shell.execute_reply.started": "2023-01-13T08:16:27.181815Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.42857142857142855,\n",
       "   'p': 0.5833333333333334,\n",
       "   'f': 0.49411764217577864},\n",
       "  'rouge-2': {'r': 0.18571428571428572,\n",
       "   'p': 0.3170731707317073,\n",
       "   'f': 0.23423422957552154},\n",
       "  'rouge-l': {'r': 0.3877551020408163,\n",
       "   'p': 0.5277777777777778,\n",
       "   'f': 0.44705881864636676}}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge \n",
    "# !pip3.9 install rouge\n",
    "\n",
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a943d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4fad695-dfbe-4b09-8d30-e2ffdf64bd79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In our current fast pace society, it is impossible to keep up with the information being generated every single minute.  \n",
    "Even if one limits itself to articles, the volume will still be too much. However, not everything contained in an article is actually relevant.\n",
    "With the abundance of information available, it is therefore neccessary to focus only on relevant information and articles.\n",
    "\n",
    "### Overall goal of the project\n",
    "Our aim is to perform abstractive and extractive text summarization on news articles. This will reduce the time spent on any given article.\n",
    "\n",
    "### What framework are you going to use (Kornia, Transformer, Pytorch-Geometrics)\n",
    "The [Transformers](https://github.com/huggingface/transformers) framework provided by HuggingFace provides high-performance NLP models suitable for a wide range of application - including text summarization.\n",
    "\n",
    "\n",
    "### What data are you going to run on (initially, may change)\n",
    "The [CNN Dailymail](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail) Dataset contains approximately 300k new articles.\n",
    "Each entry contains the article alongside the summarized article, as well as a unique id.\n",
    "If time allows, we may expand our model by the [XSum dataset and additional articles from Multi-News](https://www.kaggle.com/datasets/sbhatti/news-summarization), available on Kaggle as well.\n",
    "\n",
    "\n",
    "\n",
    "### What deep learning models do you expect to use\n",
    "Due to both time- and computational constraints, we will refer to pre-trained models, which we intend to fine-tune on the dataset.\n",
    "As the dataset is fairly popular for text summarization, there are several models fitted to it already available. We will use [BigBirdPegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus) or [Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus), and might extend using [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) or [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b75d02-c583-4242-9be1-6104436de440",
   "metadata": {},
   "source": [
    "## Êï∞ÊçÆÈõÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fdf124-a0fa-4b93-8cab-700283f38c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:27.202498Z",
     "iopub.status.busy": "2023-01-13T08:16:27.202330Z",
     "iopub.status.idle": "2023-01-13T08:16:28.042287Z",
     "shell.execute_reply": "2023-01-13T08:16:28.040969Z",
     "shell.execute_reply.started": "2023-01-13T08:16:27.202482Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# https://www.kaggle.com/datasets/sbhatti/news-summarization\n",
    "data = pd.read_csv('data.csv', nrows=10000)\n",
    "data = data[['Content', 'Summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eddbab3-cca3-472c-bb4b-d7e7ab9b6251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:28.043265Z",
     "iopub.status.busy": "2023-01-13T08:16:28.043135Z",
     "iopub.status.idle": "2023-01-13T08:16:28.050658Z",
     "shell.execute_reply": "2023-01-13T08:16:28.050068Z",
     "shell.execute_reply.started": "2023-01-13T08:16:28.043248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York police are concerned drones could bec...</td>\n",
       "      <td>Police have investigated criminals who have ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By . Ryan Lipman . Perhaps Australian porn sta...</td>\n",
       "      <td>Porn star Angela White secretly filmed sex act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This was, Sergio Garcia conceded, much like be...</td>\n",
       "      <td>American draws inspiration from fellow country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Ebola outbreak that began in Guinea four mo...</td>\n",
       "      <td>World Health Organisation: 635 infections and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By . Associated Press and Daily Mail Reporter ...</td>\n",
       "      <td>A sinkhole opened up at 5:15am this morning in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  New York police are concerned drones could bec...   \n",
       "1  By . Ryan Lipman . Perhaps Australian porn sta...   \n",
       "2  This was, Sergio Garcia conceded, much like be...   \n",
       "3  An Ebola outbreak that began in Guinea four mo...   \n",
       "4  By . Associated Press and Daily Mail Reporter ...   \n",
       "\n",
       "                                             Summary  \n",
       "0  Police have investigated criminals who have ri...  \n",
       "1  Porn star Angela White secretly filmed sex act...  \n",
       "2  American draws inspiration from fellow country...  \n",
       "3  World Health Organisation: 635 infections and ...  \n",
       "4  A sinkhole opened up at 5:15am this morning in...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a231333-5adc-43cb-833f-ef065057b571",
   "metadata": {},
   "source": [
    "## Ê®°ÂûãÂä†ËΩΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d43e16-a84a-4b82-88d7-1c4280f8b4db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:28.056515Z",
     "iopub.status.busy": "2023-01-13T08:16:28.056336Z",
     "iopub.status.idle": "2023-01-13T08:16:28.139404Z",
     "shell.execute_reply": "2023-01-13T08:16:28.138206Z",
     "shell.execute_reply.started": "2023-01-13T08:16:28.056498Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99524578-efdb-40e0-be78-75d31a6c21f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:28.142150Z",
     "iopub.status.busy": "2023-01-13T08:16:28.141567Z",
     "iopub.status.idle": "2023-01-13T08:16:35.513877Z",
     "shell.execute_reply": "2023-01-13T08:16:35.513340Z",
     "shell.execute_reply.started": "2023-01-13T08:16:28.142078Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# transformers ‰ªéÁΩëÁ´ô‰∏äËá™Âä®‰∏ãËΩΩÊùÉÈáçÁöÑËØçÂÖ∏\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2350cea1-cf4c-45e8-8268-0cc5f8b33124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:35.514877Z",
     "iopub.status.busy": "2023-01-13T08:16:35.514696Z",
     "iopub.status.idle": "2023-01-13T08:16:41.601427Z",
     "shell.execute_reply": "2023-01-13T08:16:41.600692Z",
     "shell.execute_reply.started": "2023-01-13T08:16:35.514854Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3704: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "d:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 64 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['She worked like crazy and completed 115 hours of community service in Brooklyn to meet her Thursday deadline and avoid jail.'],\n",
       " \"‚Äì Tomorrow looks to be a milestone day for Lindsay Lohan: Her lawyer will be able to report to a Los Angeles judge that she has completed all her necessary community service, paving the way for her to be off probation for the first time in seven years, reports TMZ. The community service stems from a reckless driving case, and things looked bleak for Lohan less than three weeks ago when a judge informed her that she still had 115 hours to complete before a May 28 deadline, notes the New York Daily News. Lohan got it done, however, and she's been posting photos of herself on the job at a women's shelter in New York City.\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer.prepare_seq2seq_batch(data['Content'].iloc[20], \n",
    "                                        truncation=True, \n",
    "                                        padding='longest', \n",
    "                                        return_tensors=\"pt\")\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "tgt_text, data['Summary'].iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9cfb776-d689-4925-8207-fbd29587201f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:41.602256Z",
     "iopub.status.busy": "2023-01-13T08:16:41.602127Z",
     "iopub.status.idle": "2023-01-13T08:16:41.607851Z",
     "shell.execute_reply": "2023-01-13T08:16:41.607526Z",
     "shell.execute_reply.started": "2023-01-13T08:16:41.602240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.5, 'p': 0.11235955056179775, 'f': 0.18348623553572932},\n",
       "  'rouge-2': {'r': 0.1, 'p': 0.017699115044247787, 'f': 0.030075185414664696},\n",
       "  'rouge-l': {'r': 0.45, 'p': 0.10112359550561797, 'f': 0.16513761168251836}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(data['Summary'].iloc[20], tgt_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26216f-ea94-48b5-a503-8548b92e0d85",
   "metadata": {},
   "source": [
    "## Êï∞ÊçÆÈõÜÊûÑÂª∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9754f80c-6587-4c71-9044-3ae95e96a00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:41.608619Z",
     "iopub.status.busy": "2023-01-13T08:16:41.608439Z",
     "iopub.status.idle": "2023-01-13T08:16:42.299360Z",
     "shell.execute_reply": "2023-01-13T08:16:42.298857Z",
     "shell.execute_reply.started": "2023-01-13T08:16:41.608604Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Ëá™ÂÆö‰πâÂØπÊñáÊú¨ÂÅöÂ§ÑÁêÜÔºåËØªÂèñÂçïÊù°ËÆ∞ÂΩï\n",
    "# „ÄêcontentÔºå summary„Äë ÊòØ‰∏ÄÊù°ËÆ∞ÂΩï\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7466c9-22ef-476b-9727-4234d7704f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:42.300238Z",
     "iopub.status.busy": "2023-01-13T08:16:42.300062Z",
     "iopub.status.idle": "2023-01-13T08:16:42.305770Z",
     "shell.execute_reply": "2023-01-13T08:16:42.305140Z",
     "shell.execute_reply.started": "2023-01-13T08:16:42.300221Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels):\n",
    "    # content\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=300)\n",
    "    \n",
    "    # summary\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True, max_length=200)\n",
    "    \n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "def prepare_data(model_name, \n",
    "                 train_texts, train_labels, \n",
    "                 val_texts=None, val_labels=None, \n",
    "                 test_texts=None, test_labels=None):\n",
    "    \"\"\"\n",
    "    Prepare input data for model fine-tuning\n",
    "    \"\"\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    prepare_val = False if val_texts is None or val_labels is None else True\n",
    "    prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "    train_dataset = tokenize_data(train_texts, train_labels)\n",
    "    val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "    test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a1204-5dda-4734-a80a-4fe5c3bba4c7",
   "metadata": {},
   "source": [
    "## Ê®°ÂûãËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b66946c-8f76-49dc-a36e-39e621aed51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:42.306594Z",
     "iopub.status.busy": "2023-01-13T08:16:42.306472Z",
     "iopub.status.idle": "2023-01-13T08:16:42.375989Z",
     "shell.execute_reply": "2023-01-13T08:16:42.375008Z",
     "shell.execute_reply.started": "2023-01-13T08:16:42.306580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "    \"\"\"\n",
    "    Prepare configurations and base model for fine-tuning\n",
    "    \"\"\"\n",
    "    # torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch_device = 'cpu'\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if freeze_encoder:\n",
    "        for param in model.model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if val_dataset is not None:\n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=output_dir,           # output directory\n",
    "          num_train_epochs=1,              # total number of training epochs\n",
    "          per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n",
    "          per_device_eval_batch_size=1,    # batch size for evaluation, can increase if memory allows\n",
    "          save_steps=500,                  # number of updates steps before checkpoint saves\n",
    "          save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "          evaluation_strategy='steps',     # evaluation strategy to adopt during training\n",
    "          eval_steps=100,                  # number of update steps before evaluation\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01,               # strength of weight decay\n",
    "          logging_dir='./logs',            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          report_to = 'wandb',\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "          model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "          args=training_args,                  # training arguments, defined above\n",
    "          train_dataset=train_dataset,         # training dataset\n",
    "          eval_dataset=val_dataset,            # evaluation dataset\n",
    "          tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=output_dir,           # output directory\n",
    "          num_train_epochs=1,           # total number of training epochs\n",
    "          per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n",
    "          save_steps=500,                  # number of updates steps before checkpoint saves\n",
    "          save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01,               # strength of weight decay\n",
    "          logging_dir='./logs',            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          report_to = 'wandb',\n",
    "          \n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "          model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "          args=training_args,                  # training arguments, defined above\n",
    "          train_dataset=train_dataset,         # training dataset\n",
    "          tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb81df9-2ee4-43ef-bd8b-baed837b485e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:42.378657Z",
     "iopub.status.busy": "2023-01-13T08:16:42.378411Z",
     "iopub.status.idle": "2023-01-13T08:22:18.913301Z",
     "shell.execute_reply": "2023-01-13T08:22:18.912882Z",
     "shell.execute_reply.started": "2023-01-13T08:16:42.378641Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './pegasus-xsum'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# use Pegasus Large model as base for fine-tuning\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./pegasus-xsum\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m train_dataset, _, _, tokenizer \u001b[39m=\u001b[39m prepare_data(model_name, train_texts, train_labels)\n\u001b[0;32m      8\u001b[0m trainer \u001b[39m=\u001b[39m prepare_fine_tuning(model_name, tokenizer, train_dataset)\n\u001b[0;32m      9\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn [11], line 18\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(model_name, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_data\u001b[39m(model_name, \n\u001b[0;32m     12\u001b[0m                  train_texts, train_labels, \n\u001b[0;32m     13\u001b[0m                  val_texts\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, val_labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \n\u001b[0;32m     14\u001b[0m                  test_texts\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, test_labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     15\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    Prepare input data for model fine-tuning\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     tokenizer \u001b[39m=\u001b[39m PegasusTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m     19\u001b[0m     prepare_val \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m val_texts \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m val_labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     prepare_test \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m test_texts \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m test_labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1760\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1758\u001b[0m             resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[0;32m   1759\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1760\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   1761\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   1762\u001b[0m             file_path,\n\u001b[0;32m   1763\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1764\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   1765\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1766\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   1767\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   1768\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1769\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   1770\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   1771\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m   1772\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1773\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1774\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m   1775\u001b[0m         )\n\u001b[0;32m   1776\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[0;32m   1778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[0;32m    110\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[0;32m    112\u001b[0m ):\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m         validate_repo_id(arg_value)\n\u001b[0;32m    116\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:172\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    167\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id:\n\u001b[0;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot have -- or .. in repo_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './pegasus-xsum'."
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = list(data['Content'].iloc[:100]), list(data['Summary'].iloc[:100])\n",
    "\n",
    "# use Pegasus Large model as base for fine-tuning\n",
    "model_name = './pegasus-xsum'\n",
    "\n",
    "train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)\n",
    "\n",
    "trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d9ec5a-644e-4fc3-ab4c-e1cb485d456d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:23:38.707726Z",
     "iopub.status.busy": "2023-01-13T08:23:38.707283Z",
     "iopub.status.idle": "2023-01-13T08:23:40.949595Z",
     "shell.execute_reply": "2023-01-13T08:23:40.949208Z",
     "shell.execute_reply.started": "2023-01-13T08:23:38.707676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/model.pt\n",
      "Configuration saved in results/model.pt/config.json\n",
      "Model weights saved in results/model.pt/pytorch_model.bin\n",
      "tokenizer config file saved in results/model.pt/tokenizer_config.json\n",
      "Special tokens file saved in results/model.pt/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('results/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd13efef-a95f-4b1d-ba22-ff0396ef648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:24:03.893450Z",
     "iopub.status.busy": "2023-01-13T08:24:03.892883Z",
     "iopub.status.idle": "2023-01-13T08:24:10.296960Z",
     "shell.execute_reply": "2023-01-13T08:24:10.296571Z",
     "shell.execute_reply.started": "2023-01-13T08:24:03.893401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file results/model.pt/added_tokens.json. We won't load it.\n",
      "Didn't find file results/model.pt/tokenizer.json. We won't load it.\n",
      "loading file results/model.pt/spiece.model\n",
      "loading file None\n",
      "loading file results/model.pt/special_tokens_map.json\n",
      "loading file results/model.pt/tokenizer_config.json\n",
      "loading file None\n",
      "loading configuration file results/model.pt/config.json\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"./pegasus-xsum\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.6,\n",
      "  \"max_length\": 64,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading weights file results/model.pt/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n",
      "\n",
      "All the weights of PegasusForConditionalGeneration were initialized from the model checkpoint at results/model.pt/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PegasusForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"results/model.pt/\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"results/model.pt/\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1261f4da-463a-47de-847c-62dc9f52cafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:24:38.793135Z",
     "iopub.status.busy": "2023-01-13T08:24:38.792559Z",
     "iopub.status.idle": "2023-01-13T08:24:42.839440Z",
     "shell.execute_reply": "2023-01-13T08:24:42.839064Z",
     "shell.execute_reply.started": "2023-01-13T08:24:38.793086Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Bordeaux clinched the French league title with a 1-0 win over Caen on a dramatic final day of the season.'],\n",
       " 'Bordeaux beat relegated Caen 1-0 to clinch the French league title .\\nMarseille finish runners-up despite 4-0 home win over Rennes .\\nAtletico Madrid clinch Champions League spot from Primera Liga .\\nBesiktas claim Turkish league title with 2-1 win over Denizlispor .')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer.prepare_seq2seq_batch(data['Content'].iloc[50], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "tgt_text, data['Summary'].iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7509c445-d163-4982-b5cc-af4498eae6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:24:46.714942Z",
     "iopub.status.busy": "2023-01-13T08:24:46.714373Z",
     "iopub.status.idle": "2023-01-13T08:24:46.726162Z",
     "shell.execute_reply": "2023-01-13T08:24:46.725520Z",
     "shell.execute_reply.started": "2023-01-13T08:24:46.714891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.5555555555555556,\n",
       "   'p': 0.29411764705882354,\n",
       "   'f': 0.3846153800887574},\n",
       "  'rouge-2': {'r': 0.2631578947368421,\n",
       "   'p': 0.1388888888888889,\n",
       "   'f': 0.18181817729586788},\n",
       "  'rouge-l': {'r': 0.4444444444444444,\n",
       "   'p': 0.23529411764705882,\n",
       "   'f': 0.30769230316568047}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(data['Summary'].iloc[50], tgt_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06f84b-808a-4b6f-ad82-4abd8c80cba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "719f98299ccca36a884f9d141671719e20aaacc8082e234f139e455a4a5b836e"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
