{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998b4acd-e9c0-4b1c-b78a-c0b6f87ddd29",
   "metadata": {},
   "source": [
    "## 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b41426-78c2-48f3-b7bf-eeefb1bbb64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:27.181991Z",
     "iopub.status.busy": "2023-01-13T08:16:27.181040Z",
     "iopub.status.idle": "2023-01-13T08:16:27.201467Z",
     "shell.execute_reply": "2023-01-13T08:16:27.200989Z",
     "shell.execute_reply.started": "2023-01-13T08:16:27.181815Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.42857142857142855,\n",
       "   'p': 0.5833333333333334,\n",
       "   'f': 0.49411764217577864},\n",
       "  'rouge-2': {'r': 0.18571428571428572,\n",
       "   'p': 0.3170731707317073,\n",
       "   'f': 0.23423422957552154},\n",
       "  'rouge-l': {'r': 0.3877551020408163,\n",
       "   'p': 0.5277777777777778,\n",
       "   'f': 0.44705881864636676}}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge \n",
    "# !pip3.9 install rouge\n",
    "\n",
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a943d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fad695-dfbe-4b09-8d30-e2ffdf64bd79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In our current fast pace society, it is impossible to keep up with the information being generated every single minute.  \n",
    "Even if one limits itself to articles, the volume will still be too much. However, not everything contained in an article is actually relevant.\n",
    "With the abundance of information available, it is therefore neccessary to focus only on relevant information and articles.\n",
    "\n",
    "### Overall goal of the project\n",
    "Our aim is to perform abstractive and extractive text summarization on news articles. This will reduce the time spent on any given article.\n",
    "\n",
    "### What framework are you going to use (Kornia, Transformer, Pytorch-Geometrics)\n",
    "The [Transformers](https://github.com/huggingface/transformers) framework provided by HuggingFace provides high-performance NLP models suitable for a wide range of application - including text summarization.\n",
    "\n",
    "\n",
    "### What data are you going to run on (initially, may change)\n",
    "The [CNN Dailymail](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail) Dataset contains approximately 300k new articles.\n",
    "Each entry contains the article alongside the summarized article, as well as a unique id.\n",
    "If time allows, we may expand our model by the [XSum dataset and additional articles from Multi-News](https://www.kaggle.com/datasets/sbhatti/news-summarization), available on Kaggle as well.\n",
    "\n",
    "\n",
    "\n",
    "### What deep learning models do you expect to use\n",
    "Due to both time- and computational constraints, we will refer to pre-trained models, which we intend to fine-tune on the dataset.\n",
    "As the dataset is fairly popular for text summarization, there are several models fitted to it already available. We will use [BigBirdPegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus) or [Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus), and might extend using [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) or [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b75d02-c583-4242-9be1-6104436de440",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fdf124-a0fa-4b93-8cab-700283f38c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:27.202498Z",
     "iopub.status.busy": "2023-01-13T08:16:27.202330Z",
     "iopub.status.idle": "2023-01-13T08:16:28.042287Z",
     "shell.execute_reply": "2023-01-13T08:16:28.040969Z",
     "shell.execute_reply.started": "2023-01-13T08:16:27.202482Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# https://www.kaggle.com/datasets/sbhatti/news-summarization\n",
    "data = pd.read_csv('data.csv', nrows=10000)\n",
    "data = data[['Content', 'Summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eddbab3-cca3-472c-bb4b-d7e7ab9b6251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:28.043265Z",
     "iopub.status.busy": "2023-01-13T08:16:28.043135Z",
     "iopub.status.idle": "2023-01-13T08:16:28.050658Z",
     "shell.execute_reply": "2023-01-13T08:16:28.050068Z",
     "shell.execute_reply.started": "2023-01-13T08:16:28.043248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York police are concerned drones could bec...</td>\n",
       "      <td>Police have investigated criminals who have ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By . Ryan Lipman . Perhaps Australian porn sta...</td>\n",
       "      <td>Porn star Angela White secretly filmed sex act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This was, Sergio Garcia conceded, much like be...</td>\n",
       "      <td>American draws inspiration from fellow country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Ebola outbreak that began in Guinea four mo...</td>\n",
       "      <td>World Health Organisation: 635 infections and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By . Associated Press and Daily Mail Reporter ...</td>\n",
       "      <td>A sinkhole opened up at 5:15am this morning in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  New York police are concerned drones could bec...   \n",
       "1  By . Ryan Lipman . Perhaps Australian porn sta...   \n",
       "2  This was, Sergio Garcia conceded, much like be...   \n",
       "3  An Ebola outbreak that began in Guinea four mo...   \n",
       "4  By . Associated Press and Daily Mail Reporter ...   \n",
       "\n",
       "                                             Summary  \n",
       "0  Police have investigated criminals who have ri...  \n",
       "1  Porn star Angela White secretly filmed sex act...  \n",
       "2  American draws inspiration from fellow country...  \n",
       "3  World Health Organisation: 635 infections and ...  \n",
       "4  A sinkhole opened up at 5:15am this morning in...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a231333-5adc-43cb-833f-ef065057b571",
   "metadata": {},
   "source": [
    "## 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d43e16-a84a-4b82-88d7-1c4280f8b4db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:28.056515Z",
     "iopub.status.busy": "2023-01-13T08:16:28.056336Z",
     "iopub.status.idle": "2023-01-13T08:16:28.139404Z",
     "shell.execute_reply": "2023-01-13T08:16:28.138206Z",
     "shell.execute_reply.started": "2023-01-13T08:16:28.056498Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99524578-efdb-40e0-be78-75d31a6c21f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:28.142150Z",
     "iopub.status.busy": "2023-01-13T08:16:28.141567Z",
     "iopub.status.idle": "2023-01-13T08:16:35.513877Z",
     "shell.execute_reply": "2023-01-13T08:16:35.513340Z",
     "shell.execute_reply.started": "2023-01-13T08:16:28.142078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongtianchi/opt/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# transformers 从网站上自动下载权重的词典\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f12731b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2350cea1-cf4c-45e8-8268-0cc5f8b33124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:35.514877Z",
     "iopub.status.busy": "2023-01-13T08:16:35.514696Z",
     "iopub.status.idle": "2023-01-13T08:16:41.601427Z",
     "shell.execute_reply": "2023-01-13T08:16:41.600692Z",
     "shell.execute_reply.started": "2023-01-13T08:16:35.514854Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mprepare_seq2seq_batch(data[\u001b[39m'\u001b[39m\u001b[39mContent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m20\u001b[39m], \n\u001b[1;32m      2\u001b[0m                                         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m      3\u001b[0m                                         padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                                         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m translated \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m      6\u001b[0m tgt_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(translated, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "batch = tokenizer.prepare_seq2seq_batch(data['Content'].iloc[20], \n",
    "                                        truncation=True, \n",
    "                                        padding='longest', \n",
    "                                        return_tensors=\"pt\")\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "tgt_text, data['Summary'].iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cfb776-d689-4925-8207-fbd29587201f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:41.602256Z",
     "iopub.status.busy": "2023-01-13T08:16:41.602127Z",
     "iopub.status.idle": "2023-01-13T08:16:41.607851Z",
     "shell.execute_reply": "2023-01-13T08:16:41.607526Z",
     "shell.execute_reply.started": "2023-01-13T08:16:41.602240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge.get_scores(data['Summary'].iloc[20], tgt_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26216f-ea94-48b5-a503-8548b92e0d85",
   "metadata": {},
   "source": [
    "## 数据集构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9754f80c-6587-4c71-9044-3ae95e96a00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:41.608619Z",
     "iopub.status.busy": "2023-01-13T08:16:41.608439Z",
     "iopub.status.idle": "2023-01-13T08:16:42.299360Z",
     "shell.execute_reply": "2023-01-13T08:16:42.298857Z",
     "shell.execute_reply.started": "2023-01-13T08:16:41.608604Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 自定义对文本做处理，读取单条记录\n",
    "# 【content， summary】 是一条记录\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7466c9-22ef-476b-9727-4234d7704f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:42.300238Z",
     "iopub.status.busy": "2023-01-13T08:16:42.300062Z",
     "iopub.status.idle": "2023-01-13T08:16:42.305770Z",
     "shell.execute_reply": "2023-01-13T08:16:42.305140Z",
     "shell.execute_reply.started": "2023-01-13T08:16:42.300221Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels):\n",
    "    # content\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=300)\n",
    "    \n",
    "    # summary\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True, max_length=200)\n",
    "    \n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "def prepare_data(model_name, \n",
    "                 train_texts, train_labels, \n",
    "                 val_texts=None, val_labels=None, \n",
    "                 test_texts=None, test_labels=None):\n",
    "    \"\"\"\n",
    "    Prepare input data for model fine-tuning\n",
    "    \"\"\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    prepare_val = False if val_texts is None or val_labels is None else True\n",
    "    prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "    train_dataset = tokenize_data(train_texts, train_labels)\n",
    "    val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "    test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a1204-5dda-4734-a80a-4fe5c3bba4c7",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66946c-8f76-49dc-a36e-39e621aed51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:42.306594Z",
     "iopub.status.busy": "2023-01-13T08:16:42.306472Z",
     "iopub.status.idle": "2023-01-13T08:16:42.375989Z",
     "shell.execute_reply": "2023-01-13T08:16:42.375008Z",
     "shell.execute_reply.started": "2023-01-13T08:16:42.306580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "    \"\"\"\n",
    "    Prepare configurations and base model for fine-tuning\n",
    "    \"\"\"\n",
    "    # torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch_device = 'cpu'\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if freeze_encoder:\n",
    "        for param in model.model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if val_dataset is not None:\n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=output_dir,           # output directory\n",
    "          num_train_epochs=1,              # total number of training epochs\n",
    "          per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n",
    "          per_device_eval_batch_size=1,    # batch size for evaluation, can increase if memory allows\n",
    "          save_steps=500,                  # number of updates steps before checkpoint saves\n",
    "          save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "          evaluation_strategy='steps',     # evaluation strategy to adopt during training\n",
    "          eval_steps=100,                  # number of update steps before evaluation\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01,               # strength of weight decay\n",
    "          logging_dir='./logs',            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          report_to = 'wandb',\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "          model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "          args=training_args,                  # training arguments, defined above\n",
    "          train_dataset=train_dataset,         # training dataset\n",
    "          eval_dataset=val_dataset,            # evaluation dataset\n",
    "          tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=output_dir,           # output directory\n",
    "          num_train_epochs=1,           # total number of training epochs\n",
    "          per_device_train_batch_size=1,   # batch size per device during training, can increase if memory allows\n",
    "          save_steps=500,                  # number of updates steps before checkpoint saves\n",
    "          save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01,               # strength of weight decay\n",
    "          logging_dir='./logs',            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          report_to = 'wandb',\n",
    "          \n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "          model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "          args=training_args,                  # training arguments, defined above\n",
    "          train_dataset=train_dataset,         # training dataset\n",
    "          tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb81df9-2ee4-43ef-bd8b-baed837b485e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:16:42.378657Z",
     "iopub.status.busy": "2023-01-13T08:16:42.378411Z",
     "iopub.status.idle": "2023-01-13T08:22:18.913301Z",
     "shell.execute_reply": "2023-01-13T08:22:18.912882Z",
     "shell.execute_reply.started": "2023-01-13T08:16:42.378641Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels = list(data['Content'].iloc[:100]), list(data['Summary'].iloc[:100])\n",
    "\n",
    "# use Pegasus Large model as base for fine-tuning\n",
    "model_name = 'google/pegasus-xsum'\n",
    "\n",
    "train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)\n",
    "\n",
    "trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9ec5a-644e-4fc3-ab4c-e1cb485d456d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:23:38.707726Z",
     "iopub.status.busy": "2023-01-13T08:23:38.707283Z",
     "iopub.status.idle": "2023-01-13T08:23:40.949595Z",
     "shell.execute_reply": "2023-01-13T08:23:40.949208Z",
     "shell.execute_reply.started": "2023-01-13T08:23:38.707676Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model('results/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13efef-a95f-4b1d-ba22-ff0396ef648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:24:03.893450Z",
     "iopub.status.busy": "2023-01-13T08:24:03.892883Z",
     "iopub.status.idle": "2023-01-13T08:24:10.296960Z",
     "shell.execute_reply": "2023-01-13T08:24:10.296571Z",
     "shell.execute_reply.started": "2023-01-13T08:24:03.893401Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"results/model.pt/\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"results/model.pt/\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261f4da-463a-47de-847c-62dc9f52cafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:24:38.793135Z",
     "iopub.status.busy": "2023-01-13T08:24:38.792559Z",
     "iopub.status.idle": "2023-01-13T08:24:42.839440Z",
     "shell.execute_reply": "2023-01-13T08:24:42.839064Z",
     "shell.execute_reply.started": "2023-01-13T08:24:38.793086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = tokenizer.prepare_seq2seq_batch(data['Content'].iloc[50], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "tgt_text, data['Summary'].iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509c445-d163-4982-b5cc-af4498eae6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T08:24:46.714942Z",
     "iopub.status.busy": "2023-01-13T08:24:46.714373Z",
     "iopub.status.idle": "2023-01-13T08:24:46.726162Z",
     "shell.execute_reply": "2023-01-13T08:24:46.725520Z",
     "shell.execute_reply.started": "2023-01-13T08:24:46.714891Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge.get_scores(data['Summary'].iloc[50], tgt_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06f84b-808a-4b6f-ad82-4abd8c80cba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "39752f593dfa13262d0b2f59a0a808e2a65d47d9c7cb309ae7d8e6b9e7564c98"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
